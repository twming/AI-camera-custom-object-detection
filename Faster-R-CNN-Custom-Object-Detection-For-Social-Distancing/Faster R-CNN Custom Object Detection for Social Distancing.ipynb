{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Faster R-CNN Custom Object Detection for Social Distancing.ipynb","provenance":[],"collapsed_sections":["oInAIQTWKE0N","a_XOQqmhv376"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arshpreetsingh134/Social-Distancing-Tool/blob/master/social_dist_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"XyoOZdPTI6GR"},"source":["# **Install package**"]},{"cell_type":"code","metadata":{"id":"meTdRCD8k7pm"},"source":["# Torch is a Tensor library like NumPy, with strong GPU support\n","# Torchvision is a library for Computer Vision that goes hand in hand with Torch\n","# CUDA is a parallel computing platform and API that allows tensor computations on an NVIDIA GPU.\n","# COCO (Common Object in Context) is a large-scale object detection, segmentation, and captioning dataset.\n","# COCO PythonAPI provides Python API that assists in loading, parsing, and visualizing the annotations in COCO.\n","# Detectron2 is Facebook AI Research's library that provides state-of-the-art object detection and segmentation algorithms. \n","# Opencv is pre-installed on colab\n","# FiftyOne is an open source ML tool created by Voxel51 that helps you build high-quality datasets and computer vision models.\n","!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","!pip install pyyaml==5.1\n","!pip install fiftyone\n","\n","import torch\n","TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n","CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n","print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n","# Install detectron2 that matches the above pytorch version\n","# See https://detectron2.readthedocs.io/tutorials/install.html for instructions\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/$CUDA_VERSION/torch$TORCH_VERSION/index.html\n","\n","# After installation, you may need to \"restart runtime\" in Colab."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cq5FCUqbJVlL"},"source":["# **Import libraries**"]},{"cell_type":"code","metadata":{"id":"BFMdrBBtm1dU"},"source":["# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import numpy as np\n","import os, json, cv2, random\n","from google.colab.patches import cv2_imshow\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data import MetadataCatalog, DatasetCatalog\n","\n","# import fiftyone\n","import fiftyone as fo\n","import fiftyone.zoo as foz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x1HOZ2TXxmhm"},"source":["# **People dataset - Get data**"]},{"cell_type":"markdown","metadata":{"id":"gNJRI-Zc0Gft"},"source":["Download 'people' dataset from COCO-2017 using fiftyone (Docs > FiftyOne Integrations > COCO Integration)"]},{"cell_type":"code","metadata":{"id":"rw8EjLrW0Ed5"},"source":["# List available zoo datasets\n","print(foz.list_zoo_datasets())\n","\n","# To download the COCO dataset for only the \"person\" classes\n","dataset = foz.load_zoo_dataset(\n","    \"coco-2017\",\n","    #split='train' #load train, val and test data by default\n","    label_types=[\"detections\"],\n","    classes=[\"person\"],\n","    only_matching=True,\n","    max_samples=300,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KzjybqF8svC5"},"source":["Register a COCO Format Dataset\n","If your instance-level (detection, segmentation, keypoint) dataset is already a json file in the COCO format, the dataset and its associated metadata can be registered easily with:"]},{"cell_type":"code","metadata":{"id":"6dv9tra2s3AJ"},"source":["from detectron2.data.datasets import register_coco_instances\n","\n","dataset_name_list = ('people_train','people_val')\n","\n","for dataset_name in dataset_name_list:\n","  if dataset_name in DatasetCatalog.list():\n","    DatasetCatalog.remove(dataset_name)\n","\n","register_coco_instances(\"people_train\", {}, \"/root/fiftyone/coco-2017/train/labels.json\", \"/root/fiftyone/coco-2017/train/data\")\n","register_coco_instances(\"people_val\", {}, \"/root/fiftyone/coco-2017/validation/labels.json\", \"/root/fiftyone/coco-2017/validation/data\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYpEbrrZwUay"},"source":["# **People dataset - Train Model**"]},{"cell_type":"markdown","metadata":{"id":"WSIMF_uhxQBJ"},"source":["Once youâ€™ve registered the dataset, there are configs you might want to change to train or evaluate on new dataset."]},{"cell_type":"code","metadata":{"id":"6IbJNtLzxD6r"},"source":["from detectron2.engine import DefaultTrainer\n","\n","cfg = get_cfg()\n","cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n","#faster_rcnn_R_50_C4_1x.yaml - in the orignal Faster R-CNN paper, without FPN\n","#faster_rcnn_X_101_32x8d_FPN_3x.yaml - better accuracy but longer training time and larger memory usage\n","cfg.DATASETS.TRAIN = (\"people_train\",)\n","cfg.DATASETS.TEST = (\"people_val\",)\n","cfg.DATALOADER.NUM_WORKERS = 4\n","cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_C4_1x.yaml\")  # Let training initialize from model zoo\n","cfg.SOLVER.IMS_PER_BATCH = 4\n","cfg.SOLVER.BASE_LR = 0.0002\n","cfg.SOLVER.MAX_ITER = 300 #increase if val mAP is still rising, decrease if overfit\n","cfg.SOLVER.STEPS = [] # do not decay learning rate\n","cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128 #64\n","cfg.MODEL.ROI_HEADS.NUM_CLASSES = 80\n","cfg.TEST.EVAL_PERIOD = 500\n","\n","os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","trainer = DefaultTrainer(cfg) \n","trainer.resume_or_load(resume=False)\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zxlBXY-9eYhW"},"source":["# Look at training curves in tensorboard:\n","%load_ext tensorboard\n","%tensorboard --logdir output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYxhp0m8evv-"},"source":["# **People dataset - Inference & Performance Evaluation**"]},{"cell_type":"code","metadata":{"id":"2B_NffX3e0d1"},"source":["# Inference should use the config with parameters that are used in training\n","cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")  # path to the model we just trained\n","cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set a testing threshold\n","predictor = DefaultPredictor(cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bodaY0PvfOoQ"},"source":["Evaluate performance using AP metric implemented in COCO API."]},{"cell_type":"code","metadata":{"id":"TDMW7f5yfUh1"},"source":["from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n","from detectron2.data import build_detection_test_loader\n","evaluator = COCOEvaluator(\"people_val\", output_dir=\"./output\")\n","val_loader = build_detection_test_loader(cfg, \"people_val\")\n","print(inference_on_dataset(predictor.model, val_loader, evaluator))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s_ujgMtMWl3L"},"source":["# **Implement Social Distancing check for a video clip**"]},{"cell_type":"markdown","metadata":{"id":"Y-fekcNfwLkP"},"source":["With Custom Objection Detection Model ready, we will use it in below implementation of 'social distancing for a video clip'"]},{"cell_type":"markdown","metadata":{"id":"4SAdJ6UtrG3I"},"source":["## **Mount drive**\n","\n"]},{"cell_type":"code","metadata":{"id":"7NkLbs2NrBOP"},"source":["# Mount drive\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","# Change work directory\n","import os\n","os.chdir('/content/gdrive/My Drive/Social_Distancing')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oInAIQTWKE0N"},"source":["## **Use pre-trained model**"]},{"cell_type":"code","metadata":{"id":"R-Avd9pFphCD"},"source":["# Use pre-trained Custom Object Detection Model\n","# The model and predictor are already defined as above, can be used directly\n","\n","# Below codes are used if need to use baseline model instead of custom model\n","# cfg = get_cfg()\n","# cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n","# cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7  # threshold to filter out low-scored bounding boxes predicted by the Fast R-CNN \n","# cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\n","# predictor = DefaultPredictor(cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imTAArDtJ1y_"},"source":["## **Read a video and convert to frames/images**"]},{"cell_type":"code","metadata":{"id":"JrgLLM9jm9lb"},"source":["# Path to video file & images folder\n","video = \"/content/gdrive/MyDrive/My video/sample.mp4\"\n","img_path = \"/content/gdrive/MyDrive/Social_Distancing/\"\n","\n","# Capture video\n","video_cap = cv2.VideoCapture(video)\n","\n","# Check if video file is opened successfully\n","if (video_cap.isOpened()== False): \n","  print(\"Error opening video file\")\n","\n","count=0\n","\n","# Convert video to frames/images\n","\n","while(video_cap.isOpened()):\n","    \n","  # Read frame by frame\n","  ret, frame = video_cap.read()\n","  if ret == True:\n","\n","    # Write each frame to image file and save to folder        \n","    cv2.imwrite(img_path+str(count)+'.png', frame) # 1.png, 2.png, etc.\n","    count+=1\n","    if(count==200): # Limit the no. of frames (otherwise take too long - 25fps)\n","      break\n","\n","  else: \n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAbTee-592ve"},"source":["## **Functions to read images, do predictions and process images**\n","\n"]},{"cell_type":"code","metadata":{"id":"SUpLrrFDhbgg"},"source":["# Function: returns the bottom center of every person's bb.\n","\n","def cal_bottom_ctr(img, person, i):\n","\n","  x1,y1,x2,y2 = person[i]\n","\n","  x = int((x1+x2)/2) #center\n","  y = int(y2) #bottom\n","  \n","  bottom_ctr = (x, y)\n","\n","  return bottom_ctr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sf1GPYf1Zd2o"},"source":["(x1,y1).................\n","\n",".................(x2,y2)"]},{"cell_type":"code","metadata":{"id":"t0OZ6YcNj8fh"},"source":["# Function: compute the Euclidian Distance.\n","\n","from scipy.spatial import distance\n","\n","def compute_distance(bottom_ctrs,num):\n","  dist = np.zeros((num,num))\n","  for i in range(num):\n","    for j in range(i+1,num):\n","      dist[i][j] = distance.euclidean(bottom_ctrs[i], bottom_ctrs[j])\n","  return dist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dHJGfvHDl6HJ"},"source":["# Function: returns people with distance<threshold\n","\n","def find_violation(dist,num,threshold):\n","  p1=[]\n","  p2=[]\n","  violation=[]\n","  for i in range(num):\n","    for j in range(i+1,num):\n","      if (dist[i][j]<threshold):\n","        p1.append(i)\n","        p2.append(j)\n","        violation.append(dist[i][j])\n","  return p1,p2,violation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CVt_5lAUoikN"},"source":["# Function: change the color of bb to Red for persons who violate social distancing\n","\n","def add_redbox(img, person, p1, p2):\n","  #nums = np.unique(p1+p2)\n","  for i in (p1+p2): # Merge, any person in either p1 or p2\n","    x1,y1,x2,y2 = person[i]\n","    _=cv2.rectangle(img, (x1,y1), (x2,y2), (255,0,0), 3) #red box\n","  return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CG1S4YqQsRzI"},"source":["# Function: read image, use model to do prediction, + other functions as above\n","\n","def find_violation_add_redbox(imgfile_name, threshold):\n","  img = cv2.imread(img_path+imgfile_name)\n","  outputs = predictor(img)\n","  classes = outputs['instances'].pred_classes.cpu().numpy()\n","  bbox = outputs['instances'].pred_boxes.tensor.cpu().numpy()\n","  person = bbox[np.where(classes==0)[0]] #choose class='person'\n","  bottom_ctrs = [cal_bottom_ctr(img, person, i) for i in range(len(person))]\n","  num = len(bottom_ctrs)\n","  dist = compute_distance(bottom_ctrs,num)\n","  p1, p2, violations = find_violation(dist,num,threshold)\n","  img = add_redbox(img, person, p1, p2)\n","  cv2.imwrite(img_path+imgfile_name,img)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LfKIFSzPb8jI"},"source":["## **Code execution for all frames/images of a video**"]},{"cell_type":"code","metadata":{"id":"w1HUXds9w88Q"},"source":["# The speed is ~1 min for 100 frames/images from the sample video (model = faster_rcnn_R_50_FPN_3x)\n","\n","import glob\n","import re\n","\n","imgfile_names = glob.glob('*.png')\n","imgfile_names.sort(key=lambda f: int(re.sub('\\D', '', f)))\n","\n","threshold = 100 # Can be fine tuned based on camera calibration\n","_=[find_violation_add_redbox(imgfile_names[i], threshold) for i in range(len(imgfile_names))]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"piCmZmP0pZVs"},"source":["## **Convert processed & annotated images back to a video**"]},{"cell_type":"code","metadata":{"id":"Q9tx1qANyDtn"},"source":["# Converting images back to a video\n","\n","images_all=[]\n","\n","for i in range(len(imgfile_names)):\n","\n","  # Read each image file\n","  img = cv2.imread(imgfile_names[i])\n","  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","  # Get image size\n","  height, width, layers = img.shape\n","  size = (width,height)\n","\n","  # Insert the image into an array\n","  images_all.append(img)\n","\n","out = cv2.VideoWriter(img_path+'sample_result1.mp4', cv2.VideoWriter_fourcc(*'DIVX'), 25, size)\n","\n","for i in range(len(imgfile_names)):\n","  out.write(images_all[i])\n","\n","out.release()"],"execution_count":null,"outputs":[]}]}